# 几何模型
## 概述
该文档围绕机器学习中的几何模型展开，系统介绍了几何模型的核心概念、典型模型（线性回归、线性分类器等）的原理、求解方法及相关应用细节，同时包含实验作业与改进方向。

### 🔍 文档核心基础概念
#### 1. 几何模型定义
利用数据的几何特征（如直线、曲线、距离等）构建的机器学习模型，核心包括线性分类器、支持向量机、最近邻算法、K均值聚类四类。

#### 2. 机器学习通用过程
- 目标：找到合适的函数 \(f(x)=\omega x + b\)，其中\(\omega\)为参数向量，\(b\)为偏置。
- 关键步骤：通过训练数据（Training Data），以“最小化损失函数（Loss Function）”为目标，更新模型参数，最终得到最优函数。
- 优化方式：分为直接求最优参数和迭代逐步寻优两种。

### 📊 典型几何模型详解
#### 1. 线性回归模型（预测任务）
- **模型形式**
  - 单变量：\(f(x)=\omega_{1}x_{1}+b\)
  - 多变量：\(f(\pmb{x})=\omega_{1}x_{1}+\omega_{2}x_{2}+\cdots+\omega_{n}x_{n}+b\)（通常令\(b=\omega_{0}x_{0}\)，统一写成\(f(x)=\omega x\)）
- **几何意义**：找到一条直线（或超平面），使所有样本点到该直线的距离之和最小。
- **求解逻辑**
  1. 目标策略：最小化误差平方，经验误差公式为\(min R_{emp}(f)=\frac{1}{M}\sum_{j=1}^{M}\frac{1}{2}(y^{(j)}-\omega x^{(j)})^{2}\)。
  2. 参数计算：对误差函数求导并令导数为0，得\(\omega=(X^{T}X)^{-1}X^{T}Y\)；若\(X\)不满秩，引入正则化项，公式变为\(\omega=(X^{T}X+\lambda I)^{-1}X^{T}Y\)（\(\lambda\)为正则化系数，需根据验证集选择）。

#### 2. 线性分类器
- **核心原理**：用超平面（多维空间）或直线（二维空间）分割两类样本，若存在这样的超平面则数据“线性可分”，超平面方程为\(\omega^{T}x + b = 0\)。
- **分类规则**：\(f(x)=\omega^{T}x + b > 0\)时样本为正例（\(x\in P\)），\(f(x) < 0\)时为反例（\(x\in N\)）。
- **常见类型**
  - 基本线性分类器：计算正、反例质心\(p=\frac{1}{n_{P}}\sum_{x\in P}x\)、\(n=\frac{1}{n_{N}}\sum_{x\in N}x\)，取\(\omega = p - n\)，\(b=-\frac{||p||^{2}-||n||^{2}}{2}\)。
  - Fisher线性分类器（LDA）：找最优投影轴，使两类样本投影后“类间距离最大、类内紧凑”，目标函数为\(max J_{F}(w)=\frac{(m_{1}-m_{2})^{2}}{s_{1}^{2}+s_{2}^{2}}\)，最终求得\(\omega = S_{w}^{-1}(M_{1}-M_{2})\)（\(S_{w}\)为类内离散度矩阵，\(M_{1}、M_{2}\)为原始维度类均值）。

#### 3. 支持向量机（SVM）
- **核心思想**：最大化两类样本间的“间隔”，间隔为\(\frac{2}{||\omega||}\)，仅间隔边界上的样本（支持向量）决定决策面。
- **求解过程**
  1. 原始问题：最小化\(\frac{1}{2}\omega^{T}\omega\)，约束条件为\(y^{(n)}(\omega^{T}x^{(n)}+b)\geq1\)（线性可分情况）。
  2. 对偶问题：引入拉格朗日乘子\(\alpha\)，转化为最小化\(\frac{1}{2}\sum_{n=1}^{N}\sum_{m=1}^{N}\alpha_{n}\alpha_{m}y^{(n)}y^{(m)}(x^{(n)})^{T}x^{(m)}-\sum_{n=1}^{N}\alpha_{n}\)，约束为\(\sum_{n=1}^{N}y^{(n)}\alpha_{n}=0\)且\(\alpha_{n}\geq0\)。
  3. 特殊情况处理：线性不可分时，可采用“核函数”映射到高维空间，或“软间隔最大化”（引入松弛变量\(\xi_{i}\)，目标函数变为\(\frac{1}{2}\omega^{T}\omega + C\sum_{i=1}^{N}\xi_{i}\)，\(C\)为惩罚系数）。

#### 4. 最近邻算法（KNN）
- **核心特性**：属于“惰性学习（Lazy Learning）”，训练阶段仅存储数据，无显式学习过程。
- **分类步骤**
  1. 对输入样本，找到与其最近的\(K\)个有标签训练样本。
  2. 按“多数投票”原则，将\(K\)个样本中占比最高的类别作为输入样本的类别。
- **关键问题**
  - \(K\)值选择：过小易受噪声影响，过大模型失去意义，通常用交叉验证确定。
  - 距离计算：有序属性用闵科夫斯基距离（\(p=1\)为曼哈顿距离，\(p=2\)为欧氏距离），无序属性用VDM（值差度量），混合属性需结合两种方法。

#### 5. K均值聚类（无监督学习）
- **任务目标**：将\(m\)个样本分成\(k\)个“紧凑且独立”的簇，最小化簇内平方误差\(E=\sum_{i=1}^{k}\sum_{x\in C_{i}}||x - \mu_{i}||_{2}^{2}\)（\(\mu_{i}\)为簇\(C_{i}\)的质心）。
- **算法流程**
  1. 随机选择\(k\)个样本作为初始质心。
  2. 计算每个样本到各质心的距离，将样本划分到最近的簇。
  3. 重新计算每个簇的质心。
  4. 重复步骤2-3，直到质心不再变化。
- **改进方向**：优化初始质心选择（按距离概率选点）、减少距离计算（利用三角形性质）、Mini Batch（用部分样本加速收敛，精度略有下降）。

### 📝 补充内容与任务
#### 1. 感知器（非纯几何模型）
- 模型形式：\(f(x)=sgn(\omega x)\)（\(sgn\)为符号函数）。
- 学习策略：优化绝对损失，通过迭代更新参数\(\Delta\omega=\eta e x\)（\(\eta\)为学习率，\(e=y - f(x)\)为误差）。
- 本质：源自神经元模型，后续在神经网络部分详细讲解。

#### 2. 作业任务
1. 实现Fisher算法：随机生成两类数据（每类200个），100个训练、100个测试，输出测试结果。
2. 实现K均值算法：从MNIST数据集取3类数据（如1、4、8），验证聚类效果并优化，提交源码与实验说明。
3. 分析K均值改进方法：给出3种大数据量下的改进方式，说明改进效果。

### 📌 文档核心总结
- 模型分类：监督学习（线性回归、线性分类器、SVM、KNN）与无监督学习（K均值聚类）。
- 共性逻辑：均基于几何特征（距离、超平面等），以“最小化损失/误差”为目标，通过最优化算法求解参数。
- 关键区别：监督学习需标签数据，无监督学习无需标签；部分模型（如KNN）无显式训练过程，部分（如线性回归）需求解参数。

|模型类型|核心公式|算法流程|关键说明|
| ---- | ---- | ---- | ---- |
|线性回归|1. 多变量模型：\(f(\pmb{x})=\omega_{1}x_{1}+\omega_{2}x_{2}+\cdots+\omega_{n}x_{n}+b\)（通常令\(b=\omega_{0}x_{0}\)，统一为\(f(x)=\omega x\)）<br>2. 经验误差：\(min R_{emp}(f)=\frac{1}{M}\sum_{j=1}^{M}\frac{1}{2}(y^{(j)}-\omega x^{(j)})^{2}\)<br>3. 参数求解：\(\omega=(X^{T}X)^{-1}X^{T}Y\)；满秩时加正则化：\(\omega=(X^{T}X+\lambda I)^{-1}X^{T}Y\)|1. 确定样本数据\((x^{(j)}, y^{(j)})\)（\(j=1,2,...,M\)）<br>2. 构建误差平方最小化的目标函数<br>3. 对目标函数求导并令导数为0，求解参数\(\omega\)；若\(X\)不满秩，引入正则化项后再求解<br>4. 得到最终线性回归模型用于预测|1. 属于监督学习中的预测任务<br>2. 几何意义是找到使所有点到直线（或超平面）距离之和最小的模型<br>3. \(\lambda\)为正则化系数，需根据验证集结果选择|
|基本线性分类器|1. 超平面方程：\(\omega^{T}x + b = 0\)<br>2. 质心计算：\(p=\frac{1}{n_{P}}\sum_{x\in P}x\)，\(n=\frac{1}{n_{N}}\sum_{x\in N}x\)<br>3. 参数求解：\(\omega = p - n\)，\(b=-\frac{\|\|p\|\|^{2}-\|\|n\|\|^{2}}{2}\)<br>4. 分类规则：\(f(x)=\omega^{T}x + b > 0\)为正例，\(f(x) < 0\)为反例|1. 区分样本集中的正例（\(P\)）和反例（\(N\)）<br>2. 分别计算正、反例的质心\(p\)和\(n\)<br>3. 根据质心求解权重向量\(\omega\)和偏置\(b\)，确定线性决策面<br>4. 对新样本，代入决策面方程判断其类别|1. 适用于线性可分的二分类任务<br>2. 直接基于几何概念（质心、距离）寻找最优分类器，无需复杂优化算法|
|Fisher线性分类器（LDA）|1. 类均值：\(M_{i}=\frac{1}{n_{i}}\sum_{x_{k}\in X_{i}}x_{k}\)（\(i=1,2\)）<br>2. 投影后均值：\(m_{i}=\omega^{T}M_{i}\)（\(i=1,2\)）<br>3. 类内离散度：\(s_{i}^{2}=\sum_{y_{k}\in Y_{i}}(y_{k}-m_{i})^{2}\)（\(i=1,2\)）<br>4. 目标函数：\(max J_{F}(w)=\frac{(m_{1}-m_{2})^{2}}{s_{1}^{2}+s_{2}^{2}}\)<br>5. 参数求解：\(\omega = S_{w}^{-1}(M_{1}-M_{2})\)（\(S_{w}\)为类内离散度矩阵）<br>6. 阈值：\(b=\frac{1}{2}(\omega^{T}M_{1}+\omega^{T}M_{2})\)|1. 计算两类样本在原始特征空间的类均值\(M_{1}\)、\(M_{2}\)和类内离散度矩阵\(S_{w}\)<br>2. 构建Fisher目标函数，通过拉格朗日乘子法求解最优投影方向\(\omega\)<br>3. 确定分类阈值\(b\)<br>4. 将样本投影到\(\omega\)方向，根据投影值与\(b\)的大小关系判断类别|1. 核心是找到最优投影轴，使类间距离最大、类内紧凑<br>2. 多分类时需映射到\(K-1\)维空间（\(K\)为类别数），运算较复杂|
|支持向量机（SVM）|1. 间隔公式：\(\frac{2}{\|\|\omega\|\|}\)<br>2. 线性可分原始问题：\(min_{\omega,b}\frac{1}{2}\omega^{T}\omega\)，约束\(y^{(n)}(\omega^{T}x^{(n)}+b)\geq1\)<br>3. 对偶问题：\(min_{\alpha}\frac{1}{2}\sum_{n=1}^{N}\sum_{m=1}^{N}\alpha_{n}\alpha_{m}y^{(n)}y^{(m)}(x^{(n)})^{T}x^{(m)}-\sum_{n=1}^{N}\alpha_{n}\)，约束\(\sum_{n=1}^{N}y^{(n)}\alpha_{n}=0\)且\(\alpha_{n}\geq0\)<br>4. 软间隔问题：\(min_{\omega,b}\frac{1}{2}\omega^{T}\omega + C\sum_{i=1}^{N}\xi_{i}\)，约束\(y^{(n)}(\omega^{T}x^{(n)}+b)\geq1-\xi_{i}\)且\(\xi_{i}\geq0\)|1. 线性可分情况：构建原始优化问题，转化为对偶问题，用二次规划（QP）求解拉格朗日乘子\(\alpha\)<br>2. 由\(\alpha\)计算\(\omega=\sum_{x^{(i)}\in S}\alpha_{i}y^{(i)}x^{(i)}\)（\(S\)为支持向量集）和\(b\)<br>3. 线性不可分情况：选择核函数映射到高维空间，或采用软间隔最大化，引入松弛变量\(\xi_{i}\)后求解<br>4. 用训练好的模型对新样本分类|1. 核心是最大化分类间隔，仅支持向量决定决策面<br>2. \(C\)为惩罚系数，控制对误分类样本的惩罚程度<br>3. 核函数选择需结合数据特征：高维数据用线性核，低维少样本用高斯核|
|K近邻算法（KNN）|1. 闵科夫斯基距离：\(d_{mink}(x,x')=(\sum_{i=1}^{n}\|x_{i}-x'_{i}\|^{p})^{\frac{1}{p}}\)（\(p=1\)为曼哈顿距离，\(p=2\)为欧氏距离）<br>2. VDM（无序属性）：\(VDM_{p}(a,b)=\sum_{i=1}^{k}\|\frac{m_{u,a,i}}{m_{u,a}}-\frac{m_{u,b,i}}{m_{u,b}}\|^{p}\)<br>3. 混合距离：\(MinkovDM_{p}(x,x')=(\sum_{i=1}^{n}\|x_{i}-x'_{i}\|^{p}+\sum_{i=1}^{m}VDM_{p}(x_{iu}-x'_{iu}))^{\frac{1}{p}}\)|1. 训练阶段：仅存储所有带标签的训练样本<br>2. 预测阶段：<br>  - 计算待分类样本与所有训练样本的距离<br>  - 选择距离最近的\(K\)个训练样本<br>  - 按“多数投票”原则，将\(K\)个样本中占比最高的类别作为待分类样本的类别|1. 属于惰性学习，无显式训练过程<br>2. \(K\)值通过交叉验证确定，平衡噪声影响与模型有效性<br>3. 距离计算方式需根据属性类型（有序/无序/混合）选择|
|K均值聚类|1. 质心计算：\(\mu_{i}=\frac{1}{\|C_{i}\|}\sum_{x\in C_{i}}x\)（\(i=1,2,...,k\)）<br>2. 簇内平方误差：\(E=\sum_{i=1}^{k}\sum_{x\in C_{i}}\|x - \mu_{i}\|_{2}^{2}\)|1. 初始化：从样本集\(D\)中随机选择\(k\)个样本作为初始质心<br>2. 聚类分配：计算每个样本到各质心的距离，将样本划分到距离最近的簇<br>3. 质心更新：对每个簇，重新计算其质心\(\mu_{i}\)<br>4. 迭代：重复步骤2-3，直到质心不再变化或达到迭代次数上限<br>5. 输出最终的簇划分结果|1. 属于无监督学习，无需样本标签<br>2. 初始质心选择影响聚类结果，可通过改进方法（如按距离概率选点）优化<br>3. 大数据量时可采用Mini Batch方式加速收敛|




## 线性回归
### 📚 线性回归的核心定位
线性回归是机器学习中用于**预测任务**的基础模型，属于监督学习范畴。它通过构建特征与目标值之间的线性关系，找到最优拟合直线（或高维空间的超平面），实现对未知数据的预测。在几何模型中，其核心思路是利用“所有点到直线的距离之和最小”这一几何意义，优化模型参数以降低预测误差🔶1-26、。


### 📐 线性回归的模型形式
根据特征数量的不同，线性回归分为单变量和多变量两种形式，核心都是通过线性组合特征来输出预测结果：
1. **单变量线性回归**：仅包含1个特征，模型公式为 \(f(x)=\omega_{1} x_{1}+b\)。其中，\(x_1\) 是输入特征，\(\omega_1\) 是特征的权重参数（控制特征对预测结果的影响程度），\(b\) 是偏置参数（调整模型的基准值）。
2. **多变量线性回归**：包含多个特征（如预测房价时的面积、楼层、房龄等），模型公式为 \(f({\pmb x})=\omega _{1}x_{1}+\omega _{2}x_{2}+\cdots +\omega _{n}x_{n}+b\)。其中，\(\pmb{x}\) 是多特征组成的向量，\(\omega_1,\omega_2,...,\omega_n\) 分别对应每个特征的权重，可统一用向量形式简化表示。


### 🎯 线性回归的目标策略：误差最小化
线性回归的核心目标是让“预测值与真实值的误差最小”，具体通过以下方式实现：
1. **数据与参数定义**：假设有 \(M\) 个训练样本 \((x^{(j)}, y^{(j)})\)（\(j=1,2,...,M\)），其中 \(x^{(j)}\) 是第 \(j\) 个样本的特征，\(y^{(j)}\) 是对应的真实目标值；待学习的参数为权重向量 \(\omega\)（包含所有特征的权重）和偏置 \(b\)🔶1-30、。
2. **损失函数选择**：采用“误差平方最小化”作为损失函数，公式为 \(min L\left(y^{(j)}, f\left(x^{(j)}\right)\right)=\frac{1}{2}\left(y^{(j)}-f\left(x^{(j)}\right)\right)^{2}\)。这里除以2是为了后续求导计算更简便，核心是通过平方放大较大误差，让模型优先降低显著偏差。
3. **经验误差优化**：实际训练中，需最小化所有样本的“经验误差”（即平均损失），公式为 \(min R_{emp }(f)=\frac{1}{M} \sum_{j=1}^{M} L\left(y^{(j)}, f\left(x^{(j)}\right)\right)\)。为简化计算，通常令 \(b=\omega_{0} x_{0}\)（可将偏置纳入权重向量），此时经验误差可转化为矩阵形式 \(min R_{emp }(f)=\frac{1}{2}(Y-\omega X)^{T}(Y-\omega X)\)（\(X\) 为样本特征矩阵，\(Y\) 为真实目标值向量）🔶1-33、🔶1-34、。


### 🔍 线性回归的参数求解方法
参数求解的核心是通过数学计算找到使损失函数最小的 \(\omega\)，具体分两种情况：
1. **直接求解（满秩/正定情况）**：对经验误差的矩阵形式求导，令导数为0，可得到权重参数的解析解：\(\omega=\left(X^{T} X\right)^{-1} X^{T} Y\)。这种方法适用于 \(X^T X\) 满秩（或正定）的情况，能直接得到唯一最优解。
2. **正则化求解（非满秩情况）**：若 \(X^T X\) 非满秩（如特征间存在多重共线性），则无唯一解，需引入正则化项。此时参数解为 \(\omega=\left(X^{T} X+\lambda I\right)^{-1} X^{T} y\)，其中 \(\lambda\) 是正则化系数（控制正则化强度，需根据验证集结果调整），\(I\) 是单位矩阵，作用是避免 \(X^T X\) 不可逆，同时防止模型过拟合🔶1-37、。


## 线性分类器
### 📌 线性分类器的核心定义与几何基础
线性分类器是用于**二分类任务**的几何模型，核心是通过“线性决策面”（二维为直线，多维为超平面）将两类样本分离。其几何本质是：用特征的线性组合（如二维中的 \(\omega_{1} x_{1}+\omega_{2} x_{2}+b=0\)）表示决策面，若存在这样的决策面能完全分开两类样本，则称数据集“线性可分”，反之则“线性不可分”🔶1-51、🔶1-52、。
- 公式中，\(x_1、x_2\) 是样本特征，\(\omega=(\omega_1,\omega_2)^T\) 是权重参数向量（决定决策面方向），\(b\) 是偏置参数（调整决策面位置），\(x=(x_1,x_2)^T\) 是样本特征向量。
- 分类规则：对于任意样本 \(x\)，若 \(f(x)=\omega^T x + b > 0\)，则判定为正例（\(x \in P\)）；若 \(f(x) < 0\)，则判定为反例（\(x \in N\)），本质是将样本映射到“正/负”两个离散值🔶1-54、🔶1-55、🔶1-58、。


### ✏️ 基本线性分类器：基于质心与法向量的求解
基本线性分类器通过计算两类样本的“质心”和“法向量”来确定决策面，步骤简单直观，适用于简单线性可分场景：
1. **计算质心**：分别求正例集合 \(P\) 和反例集合 \(N\) 的质心（即样本特征的平均值），公式为 \(p=\frac{1}{n_{P}} \sum_{x \in P} x\)（正例质心）、\(n=\frac{1}{n_{N}} \sum_{x \in N} x\)（反例质心），其中 \(n_P、n_N\) 分别是正、反例的样本数量🔶1-71、。
2. **确定法向量与决策面**：以“两质心连线”的方向作为决策面的法向量，即 \(\omega = p - n\)；由于决策面必然经过两质心的中点 \((p+n)/2\)，代入点法式方程可求得偏置 \(b=-\left(\| p\| ^{2}-\| n\| ^{2}\right) / 2\)🔶1-71、。
- 示例：若正例 \(x_p=(1,1)^T\)、反例 \(x_n=(-1,-1)^T\)，则质心 \(p=(1,1)^T\)、\(n=(-1,-1)^T\)，法向量 \(\omega=(2,2)^T\)，偏置 \(b=0\)，最终决策面方程为 \(f(x)=x_1 + x_2\)🔶1-61、🔶1-63、🔶1-66、🔶1-67、。


### 📊 Fisher线性分类器：基于投影优化的分类
Fisher线性分类器（又称线性判别分析LDA）不直接找决策面，而是通过“优化投影轴”提升分类效果，核心思路是让两类样本在投影后“类间距离最大、类内紧凑度最高”：
1. **投影与关键指标定义**：
   - 将高维样本 \(x\) 投影到一维轴上，投影公式为 \(y = \omega^T x\)（\(\omega\) 是投影轴方向向量）。
   - 定义原始空间中两类样本的均值 \(M_1=\frac{1}{n_1}\sum_{x \in X_1}x\)、\(M_2=\frac{1}{n_2}\sum_{x \in X_2}x\)（\(X_1、X_2\) 是两类样本集，\(n_1、n_2\) 是样本数），投影后的均值 \(m_1=\omega^T M_1\)、\(m_2=\omega^T M_2\)🔶1-90、🔶1-91、。
   - 定义投影后的类内离散度 \(s_1^2=\sum_{y \in Y_1}(y - m_1)^2\)、\(s_2^2=\sum_{y \in Y_2}(y - m_2)^2\)（\(Y_1、Y_2\) 是投影后的样本集）🔶1-92、。
2. **优化准则函数**：构造准则函数 \(max J_F(\omega)=\frac{(m_1 - m_2)^2}{s_1^2 + s_2^2}\)，目标是最大化“投影后类间距离的平方”与“投影后类内离散度之和”的比值🔶1-92、。
3. **求解投影轴与决策阈值**：
   - 通过拉格朗日乘子法求解准则函数，最终得到投影轴方向 \(\omega = S_w^{-1}(M_1 - M_2)\)，其中 \(S_w\) 是原始空间的类内离散度矩阵（需计算矩阵逆）🔶1-105、🔶1-106、。
   - 分类阈值 \(b=\frac{1}{2}(\omega^T M_1 + \omega^T M_2)\)，若投影值 \(y=\omega^T x > b\) 则判定为1类，若 \(y < b\) 则判定为2类🔶1-107、。
4. **多分类扩展**：若要处理多分类任务，可将样本映射到 \(K-1\) 维空间（\(K\) 是类别数），但运算复杂度较高，可参考《Pattern Recognition and Machine Learning》进一步学习🔶1-111、🔶1-112、🔶1-113、。


### 📌 线性分类器的补充说明
1. **与感知器的区别**：感知器虽也是线性二分类模型（公式 \(f(x)=sgn(\omega x)\)），但机理源自神经元模型，通过迭代优化绝对损失实现分类，不属于几何模型范畴，后续会在神经网络部分详细讲解🔶1-158、🔶1-170、🔶1-172、。
2. **几何工具支撑**：线性分类器的求解依赖基础几何公式，如“点到直线的距离公式”（\(d=\frac{|\omega_1 x_1 + \omega_2 x_2 + b|}{\sqrt{\omega_1^2 + \omega_2^2}}\)）、“直线点法式方程”（\(\omega_1(x_1 - x_{01}) + \omega_2(x_2 - x_{02})=0\)），这些工具为决策面的计算提供了理论基础🔶1-78、🔶1-81、。

## 支持向量机
### 🎯 支持向量机（SVM）的核心定义与核心思想
支持向量机（SVM）是线性分类模型的一种，核心目标是**最大化分类间隔**——找到一条线性决策面，使决策面到两类样本中“最近样本点”的距离最大，这些“最近样本点”被称为**支持向量**，是决定决策面位置的关键🔶1-121、。
- 决策面与间隔：假设决策面方程为 \(\omega^T x + b = 0\)，则两类样本对应的“间隔边界”分别为 \(\omega^T x + b = 1\) 和 \(\omega^T x + b = -1\)，两边界之间的间隔计算公式为 \(\frac{2}{\|\omega\|}\)（\(\|\omega\|\) 是权重向量的模），最大化间隔等价于最小化 \(\frac{1}{2}\omega^T \omega\)🔶1-121、🔶1-124、。


### 🔍 线性可分场景：SVM的原始问题与对偶求解
当数据集线性可分时，SVM通过“约束优化”求解最优决策面，为简化计算通常转化为对偶问题处理：
1. **原始优化问题**：在“所有样本都正确分类”的约束下，最小化 \(\frac{1}{2}\omega^T \omega\)。约束条件为 \(y^{(n)}(\omega^T x^{(n)} + b) \geq 1\)（\(n=1,2,...,N\)，\(y^{(n)}\) 是样本 \(x^{(n)}\) 的标签，取值为±1），确保所有样本都在间隔边界外侧🔶1-124、。
2. **对偶问题转化**：引入拉格朗日乘子 \(\alpha=(\alpha_1,\alpha_2,...,\alpha_N)\)（\(\alpha_n \geq 0\)），将原始问题转化为对偶问题：最小化 \(\frac{1}{2}\sum_{n=1}^{N}\sum_{m=1}^{N}\alpha_n \alpha_m y^{(n)} y^{(m)}(x^{(n)})^T x^{(m)} - \sum_{n=1}^{N}\alpha_n\)，约束条件为 \(\sum_{n=1}^{N} y^{(n)}\alpha_n = 0\)🔶1-125、。
3. **求解结果与参数计算**：对偶问题可通过二次规划（QP）工具求解，最终会发现大部分 \(\alpha_n = 0\)，仅支持向量对应的 \(\alpha_n > 0\)。最优权重 \(\omega\) 和偏置 \(b\) 可通过支持向量计算：
   - \(\omega = \sum_{x^{(i)} \in S} \alpha_i y^{(i)} x^{(i)}\)（\(S\) 是支持向量集合）；
   - \(b\) 由支持向量满足的 \(y^{(i)}(\omega^T x^{(i)} + b) = 1\) 推导得出🔶1-143、🔶1-144、。


### 🧩 线性不可分场景：SVM的解决方案
当数据集线性不可分时，SVM通过“核函数”或“软间隔最大化”两种方式处理，突破线性可分的限制：
1. **核函数法：升维实现可分**：核心思路是将原始低维特征空间的样本，映射到高维特征空间，使原本线性不可分的数据在高维空间中线性可分。由于直接映射计算复杂，核函数可直接计算高维空间中样本的内积，常用核函数选择原则如下🔶1-147、🔶1-148、🔶1-150、🔶1-151、🔶1-152、：
   - 高维特征、多样本：选线性核函数；
   - 低维特征、多样本：手动增特征后选线性核函数；
   - 低维特征、少样本：选高斯核函数。
2. **软间隔最大化：允许少量误差**：当完全线性可分难度大时，允许部分样本不满足“间隔约束”（即少量样本分错或在间隔内），通过引入“松弛变量”\(\xi_i \geq 0\) 调整优化目标：
   - 新优化目标：最小化 \(\frac{1}{2}\omega^T \omega + C\sum_{i=1}^{N}\xi_i\)（\(C\) 是惩罚系数，\(C\) 越大对分错样本的惩罚越重）；
   - 约束条件：\(y^{(n)}(\omega^T x^{(n)} + b) \geq 1 - \xi_i\)（\(n=1,2,...,N\)），\(\xi_i \geq 0\)🔶1-154、。


### 📌 SVM的关键总结与参考资源
SVM的核心优势在于“最大化间隔”带来的强泛化能力，其求解过程围绕“优化问题”展开，线性可分与不可分场景对应不同解决方案。若需深入了解求解细节，可参考李航《统计学习方法》或吴恩达的相关课程，其中对软间隔最大化的具体推导、核函数的数学原理有更详细讲解🔶1-149、🔶1-155、。

## 最近邻算法
### 📌 最近邻算法的核心定位与“距离”基础
最近邻算法（以K近邻KNN为代表）是基于“样本相似性”的分类模型，核心逻辑是“近朱者赤，近墨者黑”——通过计算新样本与训练样本的“距离”，判断新样本的类别。其中，“距离”是衡量相似性的关键，需满足非负性、同一性、对称性和三角不等式四大性质🔶1-198、🔶1-206、🔶1-207、🔶1-208、🔶1-209、。


### 📏 常用的距离计算方法
根据样本属性类型（有序/无序），文档介绍了不同的距离计算方式，确保相似性衡量的准确性：
1. **有序属性：闵科夫斯基距离**  
   适用于数值型特征（如身高、体重），公式为 \(d_{mink }\left(x, x'\right)=\left(\sum_{i=1}^{n}\left|x_{i}-x'_{i}\right|^{p}\right)^{1 / p}\)，其中 \(p\) 为参数：
   - 当 \(p=1\) 时，为**曼哈顿距离**（计算各维度差值的绝对值之和，类似“走街道的最短路径”）；
   - 当 \(p=2\) 时，为**欧氏距离**（计算两点间的直线距离，最常用的距离形式）🔶1-212、🔶1-213、。

2. **无序属性：VDM距离**  
   适用于非数值型特征（如西瓜颜色、花纹），通过计算“属性取值在不同类别中的比例差异”衡量距离，公式为 \(V D M_{p}(a, b)=\sum_{i=1}^{k}\left|\frac{m_{u, a, i}}{m_{u, a}}-\frac{m_{u, b, i}}{m_{u, b}}\right|^{p}\)：
   - \(m_{u,a}\) 表示属性 \(u\) 取值为 \(a\) 的总样本数；
   - \(m_{u,a,i}\) 表示第 \(i\) 类中属性 \(u\) 取值为 \(a\) 的样本数；
   - 示例：若“花纹西瓜”在“甜”类中占比70%，“非花纹西瓜”在“甜”类中占比10%，则二者在该类别中的比例差异会增大VDM距离，说明属性取值对分类有意义🔶1-215、🔶1-216、🔶1-218、。

3. **混合属性：混合度量距离**  
   当样本同时包含有序和无序属性时，将两种距离结合计算，公式为 \(MinkovDM_{p}(x,x^{\prime })=\left( \sum _{i=1}^{n}|x_{i}-{x^{\prime }}_{i}|^{p}+\sum_{i=1}^{m} V D M_{p}\left(x_{i u}-x'_{i u}\right)\right)^{1 / p}\)，可根据属性重要性添加权重调整影响程度🔶1-220、🔶1-221、。


### 🧰 K近邻（KNN）算法的核心流程
KNN是最近邻算法的典型实现，属于“懒惰学习”（训练阶段无显式学习过程，仅存储数据），核心分为训练和预测两个阶段：
1. **训练阶段：存储数据**  
   无需对数据进行复杂处理，仅将带标签的训练样本存储起来，不计算任何模型参数，是其区别于线性分类器、SVM的关键特点🔶1-225、。

2. **预测阶段：3步确定类别**  
   1. 选择参数 \(K\)（需确定“参考多少个邻居”）；
   2. 计算新样本与所有训练样本的距离，筛选出“距离最近的 \(K\) 个样本”（即K个邻居）；
   3. 按“多数投票”原则确定新样本类别：统计K个邻居中占比最高的类别，将其作为新样本的预测类别🔶1-226、🔶1-227、🔶1-228、。


### ⚠️ KNN算法的关键问题与解决思路
实际使用KNN时，需重点关注两个核心问题，直接影响模型效果：
1. **K值的选择**  
   - K值过小：仅参考少量邻居，易受噪声样本影响（如异常点可能导致分类错误）；
   - K值过大：参考过多邻居，可能包含无关样本，导致模型“泛化过度”（如极端情况K=总样本数，分类结果永远是样本数最多的类别，失去意义）；
   - 解决方法：通过交叉验证（如K-fold cross validation）测试不同K值的效果，选择分类准确率最高的K值🔶1-237、。

2. **快速K近邻搜索**  
   当训练样本量极大时，逐一计算新样本与所有训练样本的距离会导致效率极低，因此“如何快速找到K个邻居”是KNN实现的核心难点，需依赖高效的搜索算法（如KD树、Ball树）优化🔶1-230、。

## K-均值聚类
### 📌 K-均值聚类的核心定位与任务目标
K-均值聚类（K-means）是**无监督学习**中的经典聚类模型，核心任务是将无标签的样本集划分成“紧凑且独立”的K个簇（样本组）。这里的“紧凑”指簇内样本相似度高，“独立”指簇间样本相似度低，最终让簇内样本到簇中心的距离总和最小🔶1-240、🔶1-241、。
- 样本与簇定义：假设共有\(m\)个样本，组成样本集\(D=\{x^{(1)},x^{(2)},...,x^{(m)}\}\)；聚类后得到\(K\)个簇，记为\(C=\{C_1,C_2,...,C_K\}\)，每个簇\(C_i\)包含若干样本🔶1-242、。


### 📐 K-均值聚类的核心概念与目标函数
1. **簇质心**：每个簇\(C_i\)的“中心”，用簇内所有样本的平均值表示，公式为\(\mu_i=\frac{1}{|C_i|}\sum_{x \in C_i}x\)（\(|C_i|\)是簇\(C_i\)中的样本数量）。质心是衡量簇内样本分布的核心指标，聚类过程中会不断更新质心位置🔶1-243、。
2. **目标函数（最小平方误差）**：聚类的目标是最小化“所有簇内样本到对应质心的欧氏距离平方和”，公式为\(E=\sum_{i=1}^{K}\sum_{x \in C_i}\|x - \mu_i\|_2^2\)。其中\(\|x - \mu_i\|_2\)是样本\(x\)到质心\(\mu_i\)的欧氏距离，\(E\)越小说明簇内样本越集中，聚类效果越好🔶1-244、。


### 🔄 K-均值聚类的核心算法流程
K-均值聚类通过“迭代更新质心与簇划分”实现目标函数最小化，属于贪心算法（每步追求局部最优，最终逼近全局最优），具体步骤如下：
1. **初始化质心**：从样本集\(D\)中随机选择\(K\)个样本，作为初始的\(K\)个簇质心（如用不同颜色标记，区分不同簇）🔶1-249、。
2. **划分样本簇**：对每个样本\(x^{(l)}\)（\(l=1,2,...,m\)），计算它到所有质心的距离，将其划分到“距离最近的质心对应的簇”中，完成一轮簇划分🔶1-250、。
3. **更新簇质心**：根据新划分的\(K\)个簇，重新计算每个簇的质心（用簇内样本平均值更新），得到新的质心位置🔶1-251、。
4. **迭代终止**：重复步骤2（划分样本）和步骤3（更新质心），直到所有簇的质心位置不再发生变化（或变化小于设定阈值），此时聚类结果稳定，算法停止🔶1-252、。


### ⚠️ K-均值聚类的关键说明与拓展
1. **NP难题特性**：理论上，最小化目标函数\(E\)是NP难题（无法通过多项式时间找到全局最优解），实际中用贪心算法得到的是“局部最优解”，初始质心的选择会影响最终聚类效果（如初始质心选得差，可能导致局部最优解较差）🔶1-248、。
2. **聚类效果衡量**：文档提到聚类的衡量标准需参考周志华《机器学习》第197-198页内容，后续会在“模型评价”部分详细讲解，核心是从“簇内紧凑度”和“簇间分离度”两个维度评估🔶1-254、🔶1-255、🔶1-256、。