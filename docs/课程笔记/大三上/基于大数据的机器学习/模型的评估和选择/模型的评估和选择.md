# 模型的评估和选择
## 一、核心基础概念
### 1. 数据集划分
- 训练集：用于模型训练的数据集，表示为\(D=\{(x^{(j)}, y^{(j)}), j=1,2,...,M\}\)。
- 测试集：用于评估模型性能的数据集，需与训练集互斥且独立同分布采样，表示为\(T=\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(N)}, y^{(N)})\}\)。
- 划分原则：保持数据分布一致性（如分类任务的类别比例），大数据场景常见比例为98%:1%:1%（训练集:验证集:测试集）。

### 2. 误差类型
- 经验误差（训练误差）：模型在训练集上的误差，反映对训练数据的拟合程度。
- 测试误差：模型在测试集上的误差，用于近似泛化误差。
- 泛化误差：模型在新样本上的误差，是评估模型性能的核心指标，目标是最小化泛化误差。

### 3. 过拟合与欠拟合
#### （1）欠拟合
- 定义：模型无法捕捉数据普遍规律，训练误差、测试误差和泛化误差均较大。
- 原因：模型复杂度不足、训练不充分、样本量过少。
- 解决方法：增加特征（组合特征、多项式特征）、降低正则化强度、提升模型复杂度（如决策树扩展分支、神经网络增加训练轮数）。

#### （2）过拟合
- 定义：模型过度学习训练样本的个性化特征（含噪声），训练误差极小但测试误差极大，泛化能力差。
- 原因：模型复杂度过高、训练数据量少且含噪声、训练过度。
- 解决方法：早停（训练精度不再提升时停止迭代）、数据集扩增、正则化（限制模型复杂度）、Dropout（随机屏蔽隐层节点）。

## 二、测试集获取方法
### 1. 留出法（hold-out）
- 核心逻辑：将数据集\(D\)划分为互斥的训练集\(S\)和测试集\(T\)（\(D=S\cup T\)，\(S\cap T=\emptyset\)），分层采样保持分布一致。
- 优缺点：简单高效，但测试集过小会导致结果不稳定，过大则降低评估保真性；需多次随机划分取平均值。

### 2. m折交叉验证法
- 核心逻辑：将\(D\)划分为\(m\)个大小相似的分层子集，依次用\(m-1\)个子集训练、1个子集测试，共进行\(m\)次实验，取测试结果平均值。
- 特例：留一法（LOO），\(m=M\)（样本数），无数据浪费但计算复杂度极高。

### 3. 自助法（Bootstrap）
- 核心逻辑：有放回采样生成与原数据集大小相同的训练集\(D'\)，未被采样的样本（约36.8%）作为测试集。
- 优缺点：适用于小数据集，可生成多个训练集；但改变数据分布，引入估计偏差，大数据场景较少使用。

## 三、模型性能度量
### 1. 回归模型度量指标
- 均方误差（MSE）：\(E_{MSE}(x,y)=\frac{1}{M}\sum_{j=1}^{M}\|H(x^{(i)})-y^{(i)}\|_2^2\)，衡量预测值与真实值的平方差均值。
- 均方根误差（RMSE）：\(\sqrt{E_{MSE}(x,y)}\)，与预测值量纲一致。
- 平均绝对误差（MAE）：对异常值不敏感，为预测值与真实值绝对差的均值。
- 平均绝对百分比误差（MAPE）：\(E_{MAPE}(x,y)=\frac{1}{M}\sum_{j=1}^{M}\frac{|H(x^{(i)})-y^{(i)}|}{|y^{(i)}|}\)，反映相对误差比例。

### 2. 分类模型度量指标
#### （1）基础指标
- 错误率：\(E(x,y)=\frac{1}{M}\sum_{i=1}^{M}I(H(x^{(i)})\neq y^{(i)})\)，分类错误样本占比。
- 精度：正确分类样本占比，与错误率互补。

#### （2）二分类核心指标（基于混淆矩阵）
- 查准率（P）：\(P=\frac{TP}{TP+FP}\)，预测为正例的样本中实际为正例的比例。
- 查全率（R）：\(R=\frac{TP}{TP+FN}\)，实际为正例的样本中被正确预测的比例。
- \(F_\beta\)度量：\(F_\beta=(1+\beta^2)\frac{P×R}{\beta^2P+R}\)，\(\beta=1\)时为\(F_1\)（P和R的调和均值），\(\beta>1\)侧重查全率，\(\beta<1\)侧重查准率。
- ROC曲线与AUC：ROC以假正例率（\(FPR=\frac{FP}{TN+FP}\)）为横轴、真正例率（\(TPR=R\)）为纵轴；AUC为ROC曲线下面积，值越大模型泛化性能越好。

#### （3）多分类扩展指标
- 宏查准率/宏查全率：各分类器P、R的均值。
- 微查准率/微查全率：基于全局TP、FP、TN、FN的均值计算，受样本量大的类别影响更大。

### 3. 聚类任务性能指标
#### （1）外部指标（与参考模型比较）
- Jaccard系数（JC）：\(JC=\frac{a}{a+b+c}\)，取值0-1，越大聚类效果越好。
- FM指数：\(FM=\sqrt{\frac{a}{a+b}·\frac{c}{a+c}}\)，取值0-1。
- Rand指数（RI）：\(RI=\frac{2(a+d)}{m(m-1)}\)，取值0-1。

#### （2）内部指标（直接评估聚类结果）
- 轮廓系数：衡量样本与自身簇的相似度（\(a(x_i)\)）与近邻簇的相似度（\(b(x_i)\)），\(s(x_i)=\frac{b(x_i)-a(x_i)}{\max(a(x_i),b(x_i))}\)，均值越接近1效果越好。
- DBI指数：衡量簇内紧凑性与簇间分离度，值越小越好。

## 四、回归模型的泛化误差分解
### 1. 分解组件
- 偏差平方（\(Bias^2(x)\)）：\((\bar{H}(x)-y)^2\)，模型期望输出与真实值的差异，反映拟合能力，偏差大对应欠拟合。
- 方差（\(Var(x)\)）：\(E[(H_{D_i}(x)-\bar{H}(x))^2]\)，不同训练集训练的模型预测值的波动，方差大对应过拟合。
- 噪声（\(\varepsilon^2\)）：\(E_D[(y_D-y)^2]\)，样本标记与真实值的固有差异，反映学习任务本身难度。

### 2. 核心关系
- 泛化误差 = 偏差平方 + 方差 + 噪声。
- 模型复杂度与误差的关系：模型越简单/训练越不充分，偏差越大、方差越小（欠拟合）；模型越复杂/训练越充分，偏差越小、方差越大（过拟合）；最优复杂度对应泛化误差最小。

## 五、核心结论
1. 模型评估的核心是通过合理划分数据集，用测试误差近似泛化误差，选择泛化性能最优的模型。
2. 过拟合与欠拟合可通过训练误差和测试误差的差异判断，结合正则化、数据扩增等方法调整。
3. 泛化性能由模型拟合能力（偏差）、数据扰动影响（方差）和任务固有难度（噪声）共同决定，需权衡偏差与方差以达到最优拟合。