# 神经网络初步
## 一、神经元模型
### 1. 生物模型
- 结构组成：树突（接收输入信号）、细胞体（信息时空整合）、轴突（传递信号）、突触（神经元连接，连接强度具可塑性）。
- 发现历程：1904年明确神经元结构，1943年MP模型抽象生物神经元结构。

### 2. 计算模型
- 核心逻辑：模拟突触兴奋/抑制（权值调节），通过求和运算整合输入，经激活函数输出结果。
- 关键组件：输入信号、权值向量、求和单元、激活函数、输出信号。

### 3. 常用激活函数
- 阶跃函数：\(v(x)=\begin{cases}1 & x \geq 0 \\ 0 & x<0\end{cases}\)，非连续不可导。
- Sigmoid函数：\(\sigma(x)=\frac{1}{1+e^{-x}}\)，导数\(\sigma'(x)=\sigma(x)[1-\sigma(x)]\)，连续可导。
- Tanh函数：\(tanh(x)=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}\)，导数\(tanh'(x)=1-(tanh(x))^2\)，连续可导。
- ReLU函数：\(f(x)=max(0, x)\)，导数\(f'(x)=\begin{cases}0 & x \leq 0 \\ 1 & x>0\end{cases}\)，简单高效。

## 二、单层神经网络（感知器）
### 1. 网络结构
- 组成部分：输入层（仅输入单元）、输出层（含计算单元和输出单元），无隐藏层。
- 模型表达式：\(O(x)=sgn(\omega^T x + b)\)，其中\(\omega\)为权值向量，\(b\)为偏置。

### 2. 表征能力
- 线性可分：二维空间中可表示线性决策面，实现简单布尔函数（如与、或）。
- 局限性：无法解决非线性可分问题（如异或（XOR））。

### 3. 学习法则
#### （1）感知器学习法则
- 初始化：用较小非零随机数初始化权值\(\omega_i\)。
- 迭代步骤：计算输出\(o(x^{(j)})\)与误差\(e^{(j)}=y^{(j)}-o(x^{(j)})\)，调整权值\(\omega:=\omega+\eta e^{(j)} x^{(j)}\)。
- 收敛条件：训练样本实际输出与期望输出完全一致（仅适用于线性可分数据）。

#### （2）Delta法则（连续感知器）
- 优化目标：最小化平方误差\(E=\frac{1}{2}\sum_{j=1}^{M}(y^{(j)}-O^{(j)})^2\)。
- 梯度计算：权值梯度\(\nabla E=-\sum_{j=1}^{M}[y^{(j)}-f(\omega x^{(j)})]f'(\omega x^{(j)})x_i^{(j)}\)。
- 权值更新：\(\Delta \omega_i=-\eta \frac{\partial E}{\partial \omega_i}\)，适用于连续输出场景。

### 4. 学习率优化
- 核心问题：学习率过大导致训练震荡，过小导致收敛缓慢。
- 自适应策略：Adagrad算法（按参数过往导数均方根调整学习率）、\(1/t\)衰减（\(\eta^t=\eta/\sqrt{t+1}\)）。
- 梯度下降变体：随机梯度下降（单样本增量更新权值，避免局部极小值）。

## 三、两层神经网络
### 1. 网络结构
- 组成部分：输入层、单隐藏层、输出层，隐藏层引入非线性变换。
- 核心突破：通过隐藏层解决单层感知器无法处理的异或问题，扩展模型表达能力。

### 2. 表达能力提升
- 隐层节点数影响：节点数增加可表示更复杂的决策边界（开域、闭域）。
- 隐层数影响：双隐层网络可通过凸域组合表示任意形状决策面，适配复杂分类/回归任务。

## 四、多层网络（BP神经网络）
### 1. 网络结构定义
- 层级划分：输入层（第0层，\(A^{(0)}=x\)）、隐藏层（1~L-1层）、输出层（第L层）。
- 关键参数：第\(l\)层单元数\(D^{(l)}\)，权值矩阵\(\omega^{(l-1)}\)（\(D^{(l)} \times D^{(l-1)}\)），偏置向量\(B^{(l-1)}\)。

### 2. 前向传播算法（预测过程）
- 核心公式：第\(l\)层线性输出\(Z^{(l)}=\omega^{(l-1)}A^{(l-1)}+B^{(l-1)}\)，激活输出\(A^{(l)}=\sigma(Z^{(l)})\)。
- 算法步骤：从输入层\(A^{(0)}=x\)开始，逐层计算激活输出，直至输出层\(A^{(L)}\)。

### 3. 代价函数
- 单样本误差：二次代价函数\(J_{\omega,B}(x^{(j)},y^{(j)})=\frac{1}{2}\left\|y^{(j)}-A^{(L)}\right\|_2^2\)。
- 整体误差：\(J_{\omega,B}(X,Y)=\frac{1}{2M}\sum_{j=1}^{M}\left\|y^{(j)}-A^{(L)}\right\|_2^2\)，优化目标为最小化整体误差。

### 4. 误差反向传播算法（BP算法）
#### （1）核心思想
- 链式法则：从输出层反向计算各层误差（敏感度向量\(\delta^{(l)}\)），推导权值和偏置的梯度，迭代更新参数。

#### （2）关键步骤
1. 输出层（第L层）：\(\delta^{(L)}=-e \circ \sigma'(Z^{(L)})\)，权值梯度\(\nabla \omega^{(L-1)}=\delta^{(L)}(A^{(L-1)})^T\)，偏置梯度\(\nabla B^{(L-1)}=\delta^{(L)}\)。
2. 隐藏层（第L-1至1层）：\(\delta^{(l)}=[(\omega^{(l)})^T \delta^{(l+1)}] \circ \sigma'(Z^{(l)})\)，权值梯度\(\nabla \omega^{(l-1)}=\delta^{(l)}(A^{(l-1)})^T\)，偏置梯度\(\nabla B^{(l-1)}=\delta^{(l)}\)。
3. 参数更新：\(\omega^{(l)}:=\omega^{(l)}-\eta \nabla \omega^{(l)}\)，\(B^{(l)}:=B^{(l)}-\eta \nabla B^{(l)}\)。
4. 迭代训练：重复前向传播与反向传播，直至误差收敛。

## 五、单隐藏层BPNN处理MNIST数据集
### 1. 数据集介绍
- 构成：训练集（6万张手写数字图片+标签）、测试集（1万张手写数字图片+标签），图片为28×28像素。
- 数据预处理：像素值归一化至[0,1]区间，输入向量为784维（28×28展开），标签采用One-Hot编码。

### 2. 模型设计
- 网络结构：输入层（784维）、隐藏层（300个单元）、输出层（10维，对应0~9数字）。
- 激活函数：隐藏层与输出层均采用Sigmoid函数。
- 正则化：引入L2正则化项\(loss=\|y-label\|^2+\gamma\|w\|^2\)，避免过拟合。

### 3. 策略与算法
- 优化策略：最小化平方残差损失函数。
- 优化算法：随机梯度下降法。

### 4. 实验结果与分析
- 错误率：6.67%，部分错误源于模型无法捕捉像素间空间关系（如数字“5”被误分为“4”“6”）。
- 局限性：简单BPNN难以提取复杂空间特征，需深度学习模型（如CNN）优化。