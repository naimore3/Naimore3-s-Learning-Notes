# 第二章：数学基础
该文档是北京邮电大学计算机学院石川教授关于数据科学数学基础课程的教学资料，围绕线性代数、概率统计、优化理论和图论基础展开，介绍核心概念、方法及应用实例。

1. **课程概述**：涵盖数据科学领域所需的数学知识，旨在帮助学生理解数学概念并应用于实际问题。
2. **线性代数**：介绍向量、矩阵的定义、运算及特殊矩阵，如对称矩阵、正定矩阵等；讲解矩阵导数；以SVD在推荐系统中的应用为例，展示其在评分预测中的作用。
3. **概率统计**：包括随机事件、概率、条件概率、事件独立性等概念；介绍随机变量及其数字特征、常见分布；阐述数理统计中的中心极限定理、参数估计与假设检验；讲解信息论中的熵和KL散度；以朴素贝叶斯算法进行文本分类为实例，展示概率统计在文本分类中的应用。
4. **优化理论**：解释优化问题的基本概念，如优化问题、凸集合、凸函数；介绍无约束、等式约束和不等约束的优化问题形式；讲解梯度下降法和牛顿法等优化方法；以SVM分类器为例，展示优化理论在分类问题中的应用。
5. **图论基础**：介绍图的基本概念，如顶点、边、度；讲解简单图、完全图等其他概念；介绍图的矩阵表示，如邻接矩阵、关联矩阵；讲解拉普拉斯矩阵与谱；以谱聚类算法为例，展示图论在社区发现等方面的应用。 

## 线性代数
该节主要介绍了线性代数的相关知识，包括向量、矩阵、矩阵导数以及利用SVD进行评分预测的实例，具体内容如下：

### **向量**
向量是由有序数组构成，用有向线段表示，有大小和方向。\(n\)维向量可表示为行向量\(a=[a_{1}, a_{2}, \cdots, a_{n}]\)，其转置\(a^{T}=[a_{1}, a_{2}, \cdots, a_{n}]^{T}\)为列向量。
### **矩阵**
- **定义**：由\(m×n\)个数构成的\(m\)行\(n\)列表格，记为\(A_{m×n}\)等。若\(m = n\)，则为\(n\)阶方阵。
- **运算**：包括相等（同型且对应元素相等）、相加（对应元素相加）、数乘（每个元素乘以常数\(k\) ）、乘法（内积，满足特定规则）、转置（行与列互换）、哈达玛积（同型矩阵对应元素相乘）、克罗内克积（按特定规则计算）。
- **特殊矩阵**：零矩阵（元素全为\(0\)）、单位矩阵（主对角线全为\(1\)）、对称矩阵（\(A^{T}=A\)）、正定矩阵（对任意非零向量\(x\)，\(x^{T}Ax>0\) ）、正交矩阵（\(A^{T}=A^{-1}\)且\(A^{T}A = AA^{T}=E\) ）。
- **性质**：矩阵的逆（\(AB = BA = E\)时，\(A\)可逆，\(B\)是\(A\)的逆矩阵\(A^{-1}\) ）、矩阵的迹（主对角线上元素之和\(tr(A)\) ）、特征值与特征向量（满足\(A\xi=\lambda\xi\) ）、矩阵的秩（线性独立的纵列的极大数目）、矩阵的奇异值（由\(A^{T}A\)的特征值确定）、矩阵范数（度量矩阵“大小”，如Frobenius范数\(\| A\| _{F}=\left(tr\left(A A^{T}\right)\right)^{\frac{1}{2}}=\left(\sum_{i=1}^{m} \sum_{j=1}^{n}\left|A_{i j}\right|^{2}\right)^{\frac{1}{2}}\) ）。
### **矩阵导数**
- **布局**：分为分子布局和分母布局，后文统一使用分子布局，即向量\(y\)对标量\(x\)求导时，\(\frac{\partial y}{\partial x}=\left[\frac{\partial y_{1}}{\partial x}, \frac{\partial y_{2}}{\partial x}, \cdots, \frac{\partial y_{m}}{\partial x}\right]^{T}\) 。
- **求导类型**：包括向量与标量间的求导（标量\(y\)关于向量\(x\)的导数为\(\frac{\partial y}{\partial x}=\left[\frac{\partial y}{\partial x_{1}}, \frac{\partial y}{\partial x_{2}}, \cdots, \frac{\partial y}{\partial x_{n}}\right]^{T}\) ）、矩阵与标量间的求导（矩阵\(Y(x)\)对标量\(x\)的导数是各元素导数组成的矩阵）、向量对向量的求导（结果为雅可比矩阵）、函数矩阵对矩阵的求导（\(F(A)\)对矩阵\(A\)的导数是一个\(mr×ns\)的矩阵）。
下面我们通过一个具体例子来详细计算函数矩阵对矩阵的导数。

#### 问题设定
设矩阵 \(A = \begin{bmatrix}a_{11}&a_{12}\\a_{21}&a_{22}\end{bmatrix}\)，函数矩阵 \(F(A)=\begin{bmatrix}a_{11}+a_{22}&a_{12}a_{21}\\a_{12}+a_{21}&a_{11}a_{22}\end{bmatrix}\)，我们要计算 \(\frac{\partial F}{\partial A}\)。

#### 计算过程
根据定义，\(\frac{\partial F}{\partial A}=\left[\begin{array}{cc}\frac{\partial F}{\partial a_{11}} & \frac{\partial F}{\partial a_{12}} \\ \frac{\partial F}{\partial a_{21}} & \frac{\partial F}{\partial a_{22}}\end{array}\right]\)，其中每个 \(\frac{\partial F}{\partial a_{ij}}\) 是一个 \(2\times2\) 的矩阵，具体计算如下：

##### 1. 计算 \(\frac{\partial F}{\partial a_{11}}\)
\(\frac{\partial F}{\partial a_{11}}=\begin{bmatrix}\frac{\partial f_{11}}{\partial a_{11}}&\frac{\partial f_{12}}{\partial a_{11}}\\\frac{\partial f_{21}}{\partial a_{11}}&\frac{\partial f_{22}}{\partial a_{11}}\end{bmatrix}\)

 - 对于 \(f_{11}=a_{11} + a_{22}\)，\(\frac{\partial f_{11}}{\partial a_{11}} = 1\)。
 - 对于 \(f_{12}=a_{12}a_{21}\)，\(\frac{\partial f_{12}}{\partial a_{11}} = 0\)。
 - 对于 \(f_{21}=a_{12}+a_{21}\)，\(\frac{\partial f_{21}}{\partial a_{11}} = 0\)。
 - 对于 \(f_{22}=a_{11}a_{22}\)，\(\frac{\partial f_{22}}{\partial a_{11}} = a_{22}\)。

所以，\(\frac{\partial F}{\partial a_{11}}=\begin{bmatrix}1&0\\0&a_{22}\end{bmatrix}\)。

##### 2. 计算 \(\frac{\partial F}{\partial a_{12}}\)
\(\frac{\partial F}{\partial a_{12}}=\begin{bmatrix}\frac{\partial f_{11}}{\partial a_{12}}&\frac{\partial f_{12}}{\partial a_{12}}\\\frac{\partial f_{21}}{\partial a_{12}}&\frac{\partial f_{22}}{\partial a_{12}}\end{bmatrix}\)

 - 对于 \(f_{11}=a_{11} + a_{22}\)，\(\frac{\partial f_{11}}{\partial a_{12}} = 0\)。
 - 对于 \(f_{12}=a_{12}a_{21}\)，\(\frac{\partial f_{12}}{\partial a_{12}} = a_{21}\)。
 - 对于 \(f_{21}=a_{12}+a_{21}\)，\(\frac{\partial f_{21}}{\partial a_{12}} = 1\)。
 - 对于 \(f_{22}=a_{11}a_{22}\)，\(\frac{\partial f_{22}}{\partial a_{12}} = 0\)。

所以，\(\frac{\partial F}{\partial a_{12}}=\begin{bmatrix}0&a_{21}\\1&0\end{bmatrix}\)。

##### 3. 计算 \(\frac{\partial F}{\partial a_{21}}\)
\(\frac{\partial F}{\partial a_{21}}=\begin{bmatrix}\frac{\partial f_{11}}{\partial a_{21}}&\frac{\partial f_{12}}{\partial a_{21}}\\\frac{\partial f_{21}}{\partial a_{21}}&\frac{\partial f_{22}}{\partial a_{21}}\end{bmatrix}\)

 - 对于 \(f_{11}=a_{11} + a_{22}\)，\(\frac{\partial f_{11}}{\partial a_{21}} = 0\)。
 - 对于 \(f_{12}=a_{12}a_{21}\)，\(\frac{\partial f_{12}}{\partial a_{21}} = a_{12}\)。
 - 对于 \(f_{21}=a_{12}+a_{21}\)，\(\frac{\partial f_{21}}{\partial a_{21}} = 1\)。
 - 对于 \(f_{22}=a_{11}a_{22}\)，\(\frac{\partial f_{22}}{\partial a_{21}} = 0\)。

所以，\(\frac{\partial F}{\partial a_{21}}=\begin{bmatrix}0&a_{12}\\1&0\end{bmatrix}\)。

##### 4. 计算 \(\frac{\partial F}{\partial a_{22}}\)
\(\frac{\partial F}{\partial a_{22}}=\begin{bmatrix}\frac{\partial f_{11}}{\partial a_{22}}&\frac{\partial f_{12}}{\partial a_{22}}\\\frac{\partial f_{21}}{\partial a_{22}}&\frac{\partial f_{22}}{\partial a_{22}}\end{bmatrix}\)

 - 对于 \(f_{11}=a_{11} + a_{22}\)，\(\frac{\partial f_{11}}{\partial a_{22}} = 1\)。
 - 对于 \(f_{12}=a_{12}a_{21}\)，\(\frac{\partial f_{12}}{\partial a_{22}} = 0\)。
 - 对于 \(f_{21}=a_{12}+a_{21}\)，\(\frac{\partial f_{21}}{\partial a_{22}} = 0\)。
 - 对于 \(f_{22}=a_{11}a_{22}\)，\(\frac{\partial f_{22}}{\partial a_{22}} = a_{11}\)。

所以，\(\frac{\partial F}{\partial a_{22}}=\begin{bmatrix}1&0\\0&a_{11}\end{bmatrix}\)。

#### 最终结果
将上述计算得到的子矩阵组合起来，可得：
\(\frac{\partial F}{\partial A}=\left[\begin{array}{cc|cc}1&0&0&a_{21}\\0&a_{22}&1&0\\\hline0&a_{12}&1&0\\1&0&0&a_{11}\end{array}\right]\)

通过这个例子，我们详细展示了如何根据定义计算函数矩阵对矩阵的导数，关键在于分别计算函数矩阵中每个元素对原矩阵每个元素的偏导数，并将它们组合成相应的子矩阵和最终的导数矩阵。 
### **实例：利用SVD进行评分预测**
- **SVD分解**：在推荐系统的协同过滤算法中，矩阵分解（如SVD）很重要。对于秩\(r>0\)的矩阵\(A\in \mathbb{R}^{m×n}\) ，存在正交矩阵\(U\)和\(V\)，使\(A = U[\begin{array}{ll}\sum & O\\O & O\end{array}]V^{T}\) ，其中\(\sum =diag(\sigma_{1}, \sigma_{2}, \cdots, \sigma_{r})\)为非零奇异值构成的对角矩阵。
- **应用示例**：通过具体矩阵\(A\)，计算出\(A^{T}A\)的特征值、特征向量，进而得到\(A\)的SVD分解结果，用于评分预测等任务。 
[超详细解释奇异值分解（SVD）【附例题和分析】](https://blog.csdn.net/forest_LL/article/details/135343642)
[奇异值分解（SVD）](https://zhuanlan.zhihu.com/p/29846048)
## 概率统计
该节主要围绕概率统计展开，涵盖随机事件与概率、条件概率与事件独立性、随机变量及其数字特征、数理统计、信息论等理论知识，并以朴素贝叶斯算法进行文本分类为例展示其应用，具体内容如下：
### **随机事件与概率**
随机现象中试验或观察可能出现的结果就是随机事件，用大写字母表示，如多次测量同一物体重量结果有差异、同一工艺生产的灯泡寿命不同等情况。概率是用\(P(A)\)表示随机事件\(A\)发生可能性大小的数值。
### **条件概率与事件独立性**
- **条件概率**：在已知事件\(A\)发生的情况下，事件\(B\)发生的概率记为\(P(B|A)\)，计算公式为\(P(B|A)=\frac{P(AB)}{P(A)}\)。通过具体例子，利用已知概率值计算复杂条件下的概率，如已知\(P(\bar{A})、P(B)、P(A\bar{B})\)，求\(P(B|A\cup\bar{B})\)。
- **事件独立性**：指一个事件发生的概率不受其他事件影响。
- **全概率公式与贝叶斯公式**：全概率公式为\(P(B)=\sum_{i = 1}^{n}P(A_{i})P(B|A_{i})\)，在多个互斥且并集为样本空间的事件\(A_{i}\)条件下，用于计算事件\(B\)的概率；贝叶斯公式为\(P(A_{j}|B)=\frac{P(A_{j})P(B|A_{j})}{\sum_{i = 1}^{n}P(A_{i})P(B|A_{i})}\)，是在已知事件\(B\)发生的条件下，求事件\(A_{j}\)发生的概率。通过口袋取球的例子，展示如何运用这两个公式进行概率计算。
### **随机变量及其数字特征**
- **随机变量**：值随机确定的变量。分为离散型随机变量（只有有限个取值）和连续型随机变量。
- **分布函数与常见分布**：随机变量\(X\)的分布函数\(F(x)=P(X\leq x)\)。常见分布包括均匀分布、二项分布、泊松分布、指数分布、正态分布、\(\chi^{2}\)分布等，且各有其随机分布列或概率密度、期望和方差。
### **数理统计**
- **中心极限定理**：揭示大量随机变量的平均结果与随机变量的分布无关，在一定条件下，大量独立随机变量以正态分布为极限。
- **参数估计与假设检验**：参数估计是根据样本估计总体分布中未知参数，有点估计和区间估计两种方式；假设检验是根据样本数据推断关于总体的假设是否成立。

|分布|随机分布列或概率密度|期望|方差|
|--|--|--|--|
|均匀分布\(U(a,b)\)|$f(x)=\frac{1}{b - a}$，\(a < x < b\)|$\frac{a + b}{2}$|$\frac{(b - a)^2}{12}$|
|二项分布\(B(n,p)\)|$P(X = k)=C_{n}^{k}p^{k}(1 - p)^{n - k}$，\(k = 0,1,\cdots,n\)|$np$|$np(1 - p)$|
|泊松分布\(P(\lambda)\)|$P(X = k)=\frac{\lambda^{k}}{k!}e^{-\lambda}$，\(k = 0,1,\cdots\)|$\lambda$|$\lambda$|
|指数分布\(E(\lambda)\)|$f(x)=\lambda e^{-\lambda x}$|$\frac{1}{\lambda}$|$\frac{1}{\lambda^{2}}$|
|正态分布\(N(\mu,\sigma^{2})\)|$f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x - \mu)^2}{2\sigma^2}}$，\(-\infty < x < +\infty$|$\mu$|$\sigma^{2}$|
|$\chi^{2}(n)$分布| - |$n$|$2n$| 

### **信息论**
- **熵**：在信息论中用于表示随机变量不确定度的度量，离散型随机变量\(X\)的熵\(H(X)=-\sum_{x\in X}p(x)\log p(x)\)。
- **KL散度**：用于描述同一变量\(x\)的两个概率分布\(P(x)\)和\(Q(x)\)的差异，可认为是使用基于\(Q\)的编码对\(P\)进行编码所需的额外字节数，公式为\(KL(P||Q)=\int_{-\infty}^{\infty}p(x)\log\frac{p(x)}{q(x)}dx\)。
### **实例：利用朴素贝叶斯算法进行文本分类**
基于N - gram语言模型假设，当\(N = 1\)时，一元语言模型下词序列出现概率为\(P(w_{1}, w_{2}, \cdots, w_{m})=\prod_{i = 1}^{m}P(w_{i})\)，其中\(P(w_{i})=\frac{C(w_{i})}{M}\)（\(C(\cdot)\)表示词在训练语料中出现的次数，\(M\)是语料库中的总字数）。通过具体语料，计算词在不同类别句子中的概率，进而对测试句子进行分类，展示朴素贝叶斯算法在文本分类中的应用。 
## 优化理论
该部分内容主要介绍了优化理论相关知识，涵盖基本概念、优化问题的一般形式、优化方法以及SVM分类器实例，具体内容如下：

1. **基本概念**
    - **优化问题**：形式化表示为$\min f(x)$ s.t.$x \in S$，旨在找到满足约束条件$x^{*} \in S$的解，使对于任意$x \in S$，都有$f(x) \geq f(x^{*})$ 。
    - **凸集合**：对于集合$S \in \mathbb{R}^{n}$，若任意$x_{1}$，$x_{2} \in S$，都满足$\lambda x_{1}+(1-\lambda) x_{2} \in S$（$\forall \lambda \in[0,1]$），则$S$为凸集合。
    - **凸函数**：在最优化问题中，目标函数常具有凸性。对于定义在非空凸集$S \in \mathbb{R}^{n}$上的函数$f$，若对任意$x_{1}$，$x_{2} \in S$，有$f\left(\alpha x_{1}+(1-\alpha) x_{2}\right) \leq \alpha f\left(x_{1}\right)+(1-\alpha) f\left(x_{2}\right)$（$\alpha \in(0,1)$），则$f$是凸集$S$的凸函数。例如，通过计算log - sum - exp函数（即softmax函数）的一阶、二阶微分，证明其为凸函数。
2. **优化问题的一般形式**
    - **无约束的优化问题**：对于具有连续一阶导数和二阶导数的一元函数$f(x)$，任务是找出最小化（或最大化）$f(x)$的解$x^{*}$，且不对$x^{*}$施加任何约束，形式为$\min_{x} f(x)$。
    - **等式约束的优化问题**：实际中常对可行域加以限制，形式化为$\min_{x} f(x)$ s.t.$g_{i}(x)=0$（$i = 1,2,\cdots,m$）。
    - **不等约束的优化问题**：等式约束无法涵盖所有问题，不等约束形式化为$\min_{x} f(x)$ s.t.$g_{i}(x)=0$（$i = 1,\cdots,m$），$h_{j}(x) \leq 0$（$j = 1,\cdots,n$）。
3. **优化方法**
    - **梯度下降法**：也叫最速下降法，是求解无约束最优化问题$\min_{x} f(x)$的常用迭代算法。通过选取适当初值$x^{(0)}$，不断迭代更新$x$的值，使目标函数极小化，直至收敛。
    - **牛顿法**：同样用于求解无约束最优化问题，具有收敛速度快的优点。
4. **实例：SVM分类器**
    - **原理**：在考虑线性可分的二分类问题时，SVM假设最远离数据点的分离超平面最优，以提升分类可信度。数据点$(x_{i}, y_{i})$距离平面$y = wx + b$的几何距离为$\gamma_{i}=y_{i}\left(\frac{w}{\| w\| } \cdot x_{i}+\frac{b}{\| w\| }\right)$ 。
    - **优化问题与求解**：SVM的优化问题是最大化该距离，引入广义拉格朗日函数$L(w, b, \alpha)=\frac{1}{2}\| w\| ^{2}-\sum_{i=1}^{N} \alpha_{i} y_{i}\left(w \cdot x_{i}+b\right)+\sum_{i=1}^{N} \alpha_{i}$ ，将问题转化为满足KKT条件的最小化广义拉格朗日函数问题，可使用数值方法求解。为简化和推广模型，利用拉格朗日对偶性推导，得到对偶问题的新形式，这样做一方面更便于求解，另一方面方便引入核函数，将SVM推广到非线性分类问题。 