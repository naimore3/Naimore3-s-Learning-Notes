# 第二章：线性代数
该文档围绕线性代数展开，内容涵盖向量空间、矩阵、欧式空间、线性变换等核心知识板块，为深入学习线性代数提供了全面的资料。

1. **向量空间与向量**：向量分为列向量和行向量，一般指列向量，其成员为分量 。向量可进行加法、数乘及线性组合运算。向量集合包含有限和无限元素的情况，n维向量空间$R^n$由所有有n个分量的向量组成。子空间需对加法和数乘封闭，由向量张成，维数由极大线性无关组个数决定。
2. **矩阵**：$m×n$矩阵由$m$行$n$列元素构成，可按行或列看作向量集合。有矩阵数乘、加法运算，特殊矩阵包括上三角、下三角、对角、单位和零矩阵。矩阵转置有多种性质，满足$A^T = A$的是对称矩阵，$A^T = -A$的是反对称矩阵。矩阵乘法有四种理解方式，方阵可逆需满足$AA^{-1}=A^{-1}A=I_n$ ，高斯 - 若当消元法可求逆矩阵，矩阵秩等于主元个数，与可逆性、方程解的情况相关。
3. **欧式空间与内积**：在实数域$R$上的线性空间$V$中，满足交换律、齐次性、分配律和正定性的二元实函数$(x, y)$为内积，定义了内积的$V$为欧氏空间。点积是常见内积形式，此外还有其他形式。引入内积可计算向量长度、夹角，施瓦兹不等式成立。
4. **矩阵乘法**：矩阵乘向量可按行或列理解，在神经网络中有应用。矩阵乘法有按行与列内积、列的线性组合、行的线性组合、多个矩阵的和这四种理解方式。
5. **四个子空间**：矩阵$A$的零空间是$Ax = 0$的解集，是子空间；列空间由列向量张成，行空间由行向量张成；行空间与零空间是正交子空间且互为正交补。
6. **线性变换**：满足特定条件的映射$T: V \to W$为线性映射，否则为仿射映射 。线性变换有核与象，且满足$dim V = dim ker(T)+dim Im(T)$。
7. **特征值与特征向量**：非零向量$x$满足$Ax = \lambda x$时，$x$是特征向量，$\lambda$是特征值。求特征向量需计算特征多项式的根，再求解相应方程。线性变换作用在特征向量上只发生缩放变换。特征值与矩阵的迹、行列式存在数量关系，正定矩阵满足特定条件。
8. **投影**：向量可向一维子空间或矩阵的列空间投影，投影向量满足特定正交关系，由此可推出投影矩阵的表达式。
9. **基与正交基**：向量空间的基需满足向量线性无关且能张成该空间，有限维向量空间基的元素数目有限。相互正交且单位化的向量构成标准正交基，可通过正交化方法将线性无关向量化为正交向量再单位化得到。
10. **范数**：范数用于度量向量和矩阵的大小。向量范数满足非负性、齐次性和三角不等式，有多种类型；矩阵范数除满足上述条件外，还需满足相容条件，包括广义范数、Frobenius范数和从属范数等。 
## 向量空间与向量
该节主要介绍了向量与向量空间的相关基础概念，具体内容如下：

1. **向量的表示与分量**：向量有列向量和行向量两种形式，在大多数场合向量一般指代列向量。向量中的成员称为分量，如向量$v=\begin{bmatrix}1\\2\\3\end{bmatrix}$，$v_1 = 1$，$v_2 = 2$，$v_3 = 3$分别是其第1、2、3个分量 。
2. **向量的可视化与运算**：分量少于四个的向量可以可视化。向量可进行数乘和加法运算，如对于向量$v=\begin{bmatrix}v_1\\v_2\end{bmatrix}$，数乘$cv=\begin{bmatrix}cv_1\\cv_2\end{bmatrix}$；向量加法如$(a_1,a_2)+(b_1,b_2)=(a_1 + b_1,a_2 + b_2)$。向量加法和数乘运算的任意组合称为向量的线性组合，例如$cv + dw = c\begin{bmatrix}1\\1\\0\end{bmatrix}+d\begin{bmatrix}0\\1\\1\end{bmatrix}=\begin{bmatrix}c\\c + d\\d\end{bmatrix}$。
3. **向量集合与向量空间**：向量集合可以包含有限个元素，如$\left\{\begin{bmatrix}1\\2\\3\end{bmatrix},\begin{bmatrix}4\\5\\6\end{bmatrix},\begin{bmatrix}6\\8\\9\end{bmatrix},\begin{bmatrix}9\\0\\2\end{bmatrix}\right\}$；也可以包含无限个元素，如$\mathcal{L}=\left\{\begin{bmatrix}x_1\\x_2\end{bmatrix}:x_1 + x_2 = 1\right\}$。$n$维向量空间$R^n$包含所有有$n$个分量的向量的集合。
4. **子空间的定义与张成**：子集$Span(v, w)=\{cv + dw|c, d\in\mathbb{R}\}$称为$v$，$w$张成的子空间。一般地，设$v_1, v_2, \cdots, v_r\in\mathbb{R}^n$，$Span(v_1, \cdots, v_r)=\{c_1v_1 + c_2v_2+\cdots + c_rv_r|c_1, c_2,\cdots,c_r\in\mathbb{R}\}$称为$\mathbb{R}^n$的由$v_1, \cdots, v_r$张成的子空间 。一个非空子集$W\subseteq\mathbb{R}^n$若对加法封闭（即若$v, w\in W$，则$v + w\in W$）且对数乘封闭（即若$v\in W$，则$cv\in W$，$\forall c\in\mathbb{R}$），则称为一个子（向量）空间。
5. **相关性与子空间的维数**：对于向量$v_1, \cdots, v_m\in\mathbb{R}^n$，若存在不全为零的$c_1, \cdots, c_m$使得$c_1v_1+\cdots + c_mv_m = 0$，即向量方程$v_1x_1+\cdots + v_mx_m = 0$有非零解，则称$v_1, \cdots, v_m$线性相关；若只有当$c_1=\cdots = c_m = 0$时$c_1v_1+\cdots + c_mv_m = 0$成立，即向量方程$v_1x_1+\cdots + v_mx_m = 0$只有零解，则称$v_1, \cdots, v_m$线性无关。设$W\subset\mathbb{R}^n$为子空间，$W$中极大线性无关组的个数称为$W$的维数，$W$的一组极大无关组为该空间的基，同时一组基也是能张成该空间所需的最少的线性无关的向量。若$v_1, \cdots, v_m$线性相关，那么$dim Span(v_1,\cdots,v_m)<m$；若$v_1, \cdots, v_m$线性无关，那么$dim Span(v_1,\cdots,v_m)=m$。 

## 矩阵
该节主要围绕矩阵的定义、表示、运算、特殊矩阵、转置、乘法、逆与秩、子空间以及与线性变换的关系等方面展开，具体内容如下：
1. **矩阵的定义与表示**：$m,n\in\mathbb{Z}_{\geq1}$时，$m×n$矩阵是由$m$行$n$列元素构成的矩形阵列，记为$A=(a_{ij})_{1\leq i\leq m,1\leq j\leq n}$，全体$m×n$矩阵的集合记为$M_{m×n}(\mathbb{R})$或$Mat_{m×n}(\mathbb{R})$。矩阵可按行或列看作向量的集合，如矩阵$A=\begin{bmatrix}1&2&3\\4&5&6\end{bmatrix}$，按行可分成两个行向量，按列可分成三个列向量。
2. **矩阵的运算**
    - **数乘与加法**：矩阵数乘是用一个数乘以矩阵的每一个元素，如$bA = 3.5\begin{bmatrix}1&2\\3&4\\5&6\end{bmatrix}=\begin{bmatrix}3.5&7.0\\10.5&14.0\\17.5&21.0\end{bmatrix}$；矩阵加法要求两个矩阵同型（行数和列数都相同），对应元素相加，如$C = A + B=\begin{bmatrix}1&2\\3&4\\5&6\end{bmatrix}+\begin{bmatrix}7&10\\8&11\\9&12\end{bmatrix}=\begin{bmatrix}8&12\\11&15\\14&18\end{bmatrix}$。
    - **乘法**：
        - **矩阵乘向量**：$Ax$按行理解是$x$中各元素与$A$的行向量对应元素乘积之和构成的向量；按列理解是$A$的列向量的线性组合。在实际应用中，如线性神经网络中，神经元之间的连接权重就可以用矩阵乘法来表示。
        - **矩阵乘矩阵**：有四种理解方式。一是$AB$的$(i,j)$元素是$A$的第$i$行与$B$的第$j$列的内积；二是将$AB$看作$B$的列向量的线性组合；三是看作$A$的行向量的线性组合；四是看作多个矩阵的和。
3. **特殊矩阵**：包括上三角矩阵（主对角线下方元素全为0）、下三角矩阵（主对角线上方元素全为0）、对角矩阵（非主对角线元素全为0）、单位矩阵（主对角线元素全为1，其余元素全为0 ，如$I_{3}=\begin{bmatrix}1&0&0\\0&1&0\\0&0&1\end{bmatrix}$）和零矩阵（所有元素全为0，如$O_{2×3}=\begin{bmatrix}0&0&0\\0&0&0\end{bmatrix}$）。
4. **矩阵的转置**：将矩阵的行和列互换得到转置矩阵，如$\begin{bmatrix}a\\b\end{bmatrix}^T=\begin{bmatrix}a&b\end{bmatrix}$ ，转置运算满足$(A^T)^T = A$，$(AB)^T = B^TA^T$，$(A + B)^T = A^T + B^T$。满足$A^T = A$的矩阵是对称矩阵，满足$A^T = -A$的矩阵是反对称矩阵。
5. **矩阵的逆与秩**
    - **逆矩阵**：对于方阵$A\in M_{n×n}(\mathbb{R})$，若存在方阵$A^{-1}$使得$AA^{-1}=A^{-1}A = I_{n}$，则$A$可逆，$A^{-1}$是$A$的逆矩阵，不可逆的矩阵称为奇异矩阵。可通过高斯 - 若当消元法求逆矩阵。
    - **秩**：矩阵主元的个数称为矩阵的秩，记为$r(A)$。$A$可逆、$r(A)=n$（$A$有$n$个主元，可交换行顺序）、对任意$b\in\mathbb{R}^{n}$，方程$Ax = b$有唯一解这三条性质等价。
6. **矩阵的四个子空间**
    - **零空间**：矩阵$A$的零空间$N(A)$是方程$Ax = 0$的解集，是一个子空间。若$A$可逆，则零空间只含零向量。
    - **列空间与行空间**：列空间$Col\ A$由矩阵$A$的列向量张成，行空间$Row\ A$由矩阵$A$的行向量张成。若矩阵$A$表示一个函数，$Col\ A$就是函数的值域。行空间与零空间是正交子空间，且行空间和零空间互为正交补。
7. **线性变换与矩阵**：线性变换$T:V\to W$（$V,W$为向量空间）满足$T(v_{1}+v_{2}) = T(v_{1})+T(v_{2})$，$T(cv_{1}) = cT(v_{1})$。矩阵变换是作用于向量的线性变换，包括旋转、伸长和缩短等效果（平移是仿射变换）。线性变换有核$ker(T)=\{v\in V|T(v)=0_{W}\}$和象$Im(T)=\{T(v)|v\in V\}$ ，且满足$dim\ V = dim\ ker(T)+dim\ Im(T)$。 

## 欧式空间
该部分内容主要围绕欧式空间、内积、点积、向量的长度和夹角展开，介绍了它们的定义和相互关系，具体如下：

1. **欧式空间**：在向量空间基础上定义了内积运算的实数域R上的线性空间V被称为欧氏空间。其中，内积是对V中任意两个向量x、y定义的二元实函数(x, y) ，需满足交换率、齐次性、分配率和正定性。
2. **点积（数量积）**：是一种常见的内积形式，在\(\mathbb{R}^{n}\)中，\(v \cdot w = v_{1}w_{1}+\cdots + v_{n}w_{n} \in \mathbb{R}\) ，但内积并不局限于点积形式，还可以有其他形式来定义不同的线性空间。
3. **长度**：引入内积的目的之一是计算向量长度。对于向量\(v \in \mathbb{R}^{n}\)，其长度\(\| v\| =\sqrt{v \cdot v}\)，且\(\|v\| = 0\)当且仅当\(v = 0\)。
4. **夹角**：通过内积还可计算向量\(v\)和\(w\)的夹角\(\theta \in[0, \pi]\)，计算公式为\(\cos \theta=\frac{v \cdot w}{\| v\| \| w\| }\) ，当\(v \cdot w = 0\)时，两向量正交或垂直。 

## 矩阵乘法
该部分内容主要介绍了矩阵乘法相关知识，包括矩阵乘向量以及矩阵乘法的四种理解方式，具体内容如下：

1. **矩阵乘向量**
    - **线性方程组角度**：以线性方程组\(\begin{cases}a_{11}x_{1}+a_{12}x_{2}+\cdots +a_{1n}x_{n}=b_{1}\\a_{21}x_{1}+a_{22}x_{2}+\cdots +a_{2n}x_{n}=b_{2}\\\cdots\\a_{m1}x_{1}+a_{m2}x_{2}+\cdots +a_{m n}x_{n}=b_{m}\end{cases}\)为基础，引入矩阵乘向量\(Ax\)。
    - **两种观点**：按列的观点，\(Ax\)是列向量的线性组合，如\(Ax = x_{1}\begin{bmatrix}a_{11}\\\vdots\\a_{m1}\end{bmatrix}+x_{2}\begin{bmatrix}a_{12}\\\vdots\\a_{m2}\end{bmatrix}+\cdots +x_{n}\begin{bmatrix}a_{1n}\\\vdots\\a_{mn}\end{bmatrix}\)；按行的观点，\(Ax\)可看成行向量和\(x^{T}\)的内积。
    - **应用**：在介绍线性神经网络时提到，矩阵乘法可用于描述神经元之间的连接权重关系，\(W_{ij}\)是连接神经元\(x_{j}\)到神经元\(y_{i}\)的权重 。
2. **矩阵乘法的四种理解方式**
    - **方法1：行列内积**：给定两个矩阵\(A\)和\(B\)，\(AB\)的\((i, j)\)元素是\(A\)的第\(i\)行和\(B\)的第\(j\)列的内积，即\(c_{ij}=a_{i1}b_{1j}+a_{i2}b_{2j}+\cdots +a_{in}b_{nj}\)。
    - **方法2：列的线性组合**：\(AB\)的结果矩阵的每一列，都是\(A\)的列向量以\(B\)对应列元素为系数的线性组合。例如，计算矩阵\(\begin{bmatrix}1&2\\3&4\\5&6\end{bmatrix}\)与\(\begin{bmatrix}-1&1\\3&2\end{bmatrix}\)的乘积时，结果矩阵的第一列是\(-1\begin{bmatrix}1\\3\\5\end{bmatrix}+3\begin{bmatrix}2\\4\\6\end{bmatrix}\)，第二列是\(1\begin{bmatrix}1\\3\\5\end{bmatrix}+2\begin{bmatrix}2\\4\\6\end{bmatrix}\) 。
    - **方法3：行的线性组合**：\(AB\)的结果矩阵的每一行，都是\(B\)的行向量以\(A\)对应行元素为系数的线性组合。
    - **方法4：多个矩阵的和**：将矩阵乘法看作多个矩阵之和，即\(\begin{bmatrix}a_{1}&a_{2}&\cdots&a_{n}\end{bmatrix}\begin{bmatrix}b_{1}^{T}\\b_{2}^{T}\\\vdots\\b_{n}^{T}\end{bmatrix}=a_{1}b_{1}^{T}+a_{2}b_{2}^{T}+\cdots +a_{n}b_{n}^{T}\) 。 

## 矩阵的逆与秩
该部分内容主要介绍了矩阵的逆与秩的相关知识，包括矩阵的逆的定义、高斯 - 若当消元法求逆矩阵以及矩阵秩的概念，具体如下：

1. **矩阵的逆**：对于方阵\(A \in M_{n×n}(\mathbb{R})\) ，若存在方阵\(A^{-1} \in M_{n×n}(\mathbb{R})\) ，使得\(AA^{-1}=A^{-1}A = I_{n}\) ，则称\(A\)是可逆的，\(A^{-1}\)为\(A\)的逆矩阵；不可逆的矩阵称为奇异矩阵。例如，矩阵\(A=\begin{bmatrix}1&2\\3&5\end{bmatrix}\) ，其逆矩阵\(B=\begin{bmatrix}-5&2\\3&-1\end{bmatrix}\) ，满足\(AB = BA=\begin{bmatrix}1&0\\0&1\end{bmatrix}\) 。
2. **高斯 - 若当消元法**：是一种求逆矩阵的方法，通过一系列初等行变换将增广矩阵\([A|I]\)（\(I\)为单位矩阵）转化为\([I|A^{-1}]\)的形式，其中每行第一个非零元素就是主元（pivot element） 。在计算过程中，逐步利用主元消去主元同列的其它非零元，最终得到逆矩阵。
3. **矩阵的秩**：矩阵\(A\)主元的个数称为矩阵\(A\)的秩，记为\(r(A)\) 。下面三条结论相互等价：
    - \(A\)可逆；
    - \(r(A)=n\)，即\(A\)有\(n\)个主元（可交换\(A\)的行的顺序）；
    - 对任意\(b \in \mathbb{R}^{n}\)，方程\(Ax = b\)有唯一解。 

## 四个子空间

