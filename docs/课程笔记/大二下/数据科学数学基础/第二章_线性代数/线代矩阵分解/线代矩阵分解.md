# 线代矩阵分解
## 概览
---
### 一段话总结
本文围绕线性代数在数据科学中的应用展开，介绍了多种矩阵分解方法及其应用。**LU分解**将可逆且消元时无行交换的矩阵分解为下三角矩阵L和上三角矩阵U；**LDU分解**在此基础上引入对角矩阵D 。**特征值分解**针对可对角化矩阵，若矩阵A有n个线性无关的特征向量，可分解为$A = X\Lambda X^{-1}$ ，对称矩阵可进一步分解为$S = Q\Lambda Q^{T}$ 。**QR分解**利用Gram - Schmidt正交化将列满秩矩阵分解为正交矩阵Q和上三角矩阵R。**奇异值分解**可处理非方阵，将矩阵$A$分解为$A = U\sum V^{T}$ ，还有紧奇异值分解和截断奇异值分解 。**主成分分析（PCA）**是一种无监督学习降维方法，通过正交变换将线性相关变量转换为线性无关的主成分，可基于协方差矩阵或相关矩阵的特征值分解实现，也可借助数据矩阵的奇异值分解算法求解。

---
![exported_image.png](exported_image.png)
---
### 详细总结
1. **LU分解**
    - **定义**：对于非奇异矩阵（可逆）A，可利用高斯消元法写成$A = LU$的形式，其中L为下三角矩阵，U为上三角矩阵 。例如，矩阵$A=\begin{bmatrix}1&2&3\\4&5&6\\8&9&7\end{bmatrix}$可分解为$A=\begin{bmatrix}1&0&0\\4&1&0\\8&\frac{7}{3}&1\end{bmatrix}\begin{bmatrix}1&2&3\\& - 3& - 6\\&& - 3\end{bmatrix}$。
    - **条件**：A必须可逆，且在高斯消元过程中没有交换两行的操作。
    - **应用**：用于解方程$Ax = b$，将其转化为$LUx = b$，先求解$Lc = b$，再求解$Ux = c$。虽然LU分解时间复杂度为$O(n^{3})$，但分解后的两个方程求解时间复杂度为$O(n^{2})$，且三角矩阵可压缩存储，节省存储空间。
2. **LDU分解**：在LU分解基础上，进一步将矩阵A分解为$A = LDU$的形式，其中D为对角矩阵。如上述矩阵A可分解为$A=\begin{bmatrix}1&0&0\\4&1&0\\8&\frac{7}{3}&1\end{bmatrix}\begin{bmatrix}1&0&0\\0& - 3&0\\0&0& - 3\end{bmatrix}\begin{bmatrix}1&2&3\\&1&2\\&&1\end{bmatrix}$。
3. **特征值分解**
    - **定义**：假设矩阵A有n个线性无关的特征向量$x_{1},...,x_{n}$，对应的特征值为$\lambda_{1},...,\lambda_{n}$，令$X = [x_{1},...,x_{n}]$为n阶可逆矩阵，则有$X^{-1}AX=\begin{bmatrix}\lambda_{1}& & \\& \ddots & \\& & \lambda_{n}\end{bmatrix}=\Lambda$，即$A = X\Lambda X^{-1}$。
    - **可对角化性质**：若A可对角化，则$A^{k}=X\Lambda^{k}X^{-1}$。
    - **应用**：以某地区城镇化速度的矩阵$A=\begin{bmatrix}0.97&0.05\\0.03&0.95\end{bmatrix}$为例，可用于分析人口分布随时间的变化，如初始人口分布为$\begin{bmatrix}0.4\\0.6\end{bmatrix}$，经过k年，人口分布为$u_{k}=A^{k}\begin{bmatrix}0.4\\0.6\end{bmatrix}$。
4. **对称矩阵的特征值分解**：存在正交矩阵Q，使得对称矩阵S满足$S = Q\Lambda Q^{-1}=Q\Lambda Q^{T}$，即S有n个相互正交的特征向量。例如，对于特征多项式$f_{S}(\lambda)=(1 - \lambda)^{2}(2+\lambda)$的对称矩阵S，其特征值为1（重数2）， - 2（重数1） ，通过求解特征方程得到相应特征向量，进而得到正交矩阵Q和对角矩阵$\Lambda$，实现分解。
5. **QR分解**：对于列满秩矩阵$A\in M_{m×3}(\mathbb{R})$，利用Gram - Schmidt正交化将A的列向量化为标准正交基$q_{1},q_{2},q_{3}$，满足特定的子空间关系，进而得到$A = QR$的分解形式，其中Q的列向量单位正交，R为上三角矩阵。若A为$n×n$可逆方阵，同样适用该分解。
6. **奇异值分解**
    - **定义**：对于矩阵$A\in M_{m×n}(\mathbb{R})$ ，存在正交矩阵$U\in M_{m×m}(\mathbb{R})$ 、$V\in M_{n×n}(\mathbb{R})$，使得$A = U\sum V^{T}$，其中$\sum\in M_{m×n}(\mathbb{R})$为（广义）对角矩阵，$\sum=\begin{bmatrix}\sigma_{1}& & & \\& \ddots & & \\& & \sigma_{r}& \end{bmatrix}$，且$\sigma_{1}\geq\sigma_{2}\geq...\geq\sigma_{r}>0$ ，$UU^{T}=I$ ，$VV^{T}=I$ 。
    - **类型**
        - **紧奇异值分解**：与原始矩阵等秩，对于$m×n$实矩阵A，秩为$rank(A)=r$，$r\leq min(m,n)$，分解为$A = U_{r}\sum_{r}V_{r}^{T}$ ，其中$U_{r}$为$m×r$矩阵，$V_{r}$为$n×r$矩阵，$\sum_{r}$为r阶对角矩阵。
        - **截断奇异值分解**：实际应用中常用，设A为$m×n$实矩阵，秩为$rank(A)=r$，且$0<k<r$，分解为$A\approx U_{k}\sum_{k}V_{k}^{T}$ ，其中$U_{k}$为$m×k$矩阵，$V_{k}$为$n×k$矩阵，$\sum_{k}$为k阶对角矩阵。
    - **应用**：矩阵的低秩近似，A可表示为多个秩一矩阵的和，如$A=\sigma_{1}u_{1}v_{1}^{T}+\sigma_{2}u_{2}v_{2}^{T}+...+\sigma_{r}u_{r}v_{r}^{T}$（紧奇异值分解），$A_{k}=\sigma_{1}u_{1}v_{1}^{T}+\sigma_{2}u_{2}v_{2}^{T}+...+\sigma_{k}u_{k}v_{k}^{T}$（截断奇异值分解） ，根据Eckart - Young - Mirsky定理，$A_{k}$是所有秩k矩阵中对A的“最佳逼近”。
7. **主成分分析（PCA）**
    - **定义**：一种常用的无监督学习方法，利用正交变换将线性相关变量表示的观测数据转换为少数线性无关变量（主成分）表示的数据，属于降维方法，用于发现数据中的基本结构。
    - **分类**
        - **总体主成分分析**：针对m维随机变量$x=(x_{1},x_{2},...,x_{m})^{T}$，涉及均值向量$\mu$、协方差矩阵$\sum$等概念。主成分需满足系数向量为单位向量、变量互不相关、方差最大等条件。第k主成分$y_{k}=\alpha_{k}^{T}x$，方差$var(y_{k})=\lambda_{k}$（$\lambda_{k}$为$\sum$的第k个特征值）。主成分的协方差矩阵是对角矩阵，方差之和等于随机变量x的方差之和。通常根据累计方差贡献率（如达到70%以上）确定主成分个数k ，还定义了主成分对原有变量的贡献率等指标。
        - **样本主成分分析**：对m维随机变量进行n次独立观测，得到样本矩阵X ，可估计样本均值、样本协方差矩阵S和样本相关矩阵R。样本主成分的定义与总体主成分类似，且总体主成分的相关定理对样本主成分依然成立。使用样本主成分时，一般对样本数据进行规范化处理，使样本协方差矩阵S变为样本相关矩阵R。
    - **算法**
        - **基于相关矩阵的特征值分解**：先对观测数据规范化处理，计算样本相关矩阵R ，求解R的特征方程得到特征值和单位特征向量，确定主成分个数k，求出k个样本主成分，计算主成分与原变量的相关系数和贡献率，最后计算样本的主成分值。例如对学生四门课程考试成绩数据进行主成分分析，可得到反映学生整体成绩和文理科成绩关系的主成分。
        - **数据矩阵的奇异值分解算法**：构造新矩阵$X'=\frac{1}{\sqrt{n - 1}}X^{T}$，其每一列均值为0，$X'^{T}X'$等于X的协方差矩阵$S_{X}$。通过对$X'$进行截断奇异值分解，V的列向量就是X的主成分，进而得到样本主成分矩阵Y。
---
### 关键问题
1. **不同矩阵分解方法的适用场景有何不同？**
    - LU分解适用于可逆且高斯消元无行交换的矩阵，主要用于解方程；特征值分解适用于可对角化矩阵，在马氏链等场景分析矩阵幂运算问题；QR分解用于列满秩矩阵，常用于数值计算等领域；奇异值分解对非方阵和方阵都适用，在数据压缩、低秩近似等方面应用广泛；主成分分析则用于数据降维、发现数据结构，处理多变量线性相关的数据。
2. **主成分分析中如何确定主成分的个数？**
    - 通常根据累计方差贡献率来确定，如取累计方差贡献率达到规定百分比（如70%）以上的主成分个数k。还可考虑主成分对各个原有变量的贡献率，综合判断保留哪些主成分能更好地保留原有变量的信息，简化问题的同时尽可能减少信息损失。
3. **奇异值分解的几何意义是什么？**
    - 从线性变换角度，m×n矩阵A表示从n维空间$\mathbb{R}^{n}$到m维空间$\mathbb{R}^{m}$的线性变换。奇异值分解将其分解为三个简单变换：一个是坐标系的旋转或反射变换（由正交矩阵V表示），一个是坐标轴的缩放变换（由对角矩阵$\sum$表示），另一个是坐标系的旋转或反射变换（由正交矩阵U表示）。任意向量$x\in\mathbb{R}^{n}$，经$V^{T}$、$\sum$、U变换后得到$Ax\in\mathbb{R}^{m}$，这清晰地解释了线性变换的内在结构。 